{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the RAdam optimizer\n",
    "\n",
    "The [Rectified Adam](https://github.com/LiyuanLucasLiu/RAdam) optimizer has shown promising results, including lower overall variance while training, and robustness to learning rate selection. Paired with the 1-cycle learning schedule, it [seems to perform extremely well](https://github.com/LiyuanLucasLiu/RAdam). Let's see if we can beat our original score of ~0.081 dex in 40 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "PATH = os.path.abspath('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>objID</th>\n",
       "      <th>oh_p50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88696</th>\n",
       "      <td>1237661069784514749</td>\n",
       "      <td>9.103453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56820</th>\n",
       "      <td>1237658300603236537</td>\n",
       "      <td>8.860247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63795</th>\n",
       "      <td>1237660613437423799</td>\n",
       "      <td>8.765317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     objID    oh_p50\n",
       "88696  1237661069784514749  9.103453\n",
       "56820  1237658300603236537  8.860247\n",
       "63795  1237660613437423799  8.765317"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_csv = f'{PATH}/catalogs/train.csv'\n",
    "df = pd.read_csv(train_label_csv)\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "class RAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.buffer = [[None, None, None] for ind in range(10)]\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = self.buffer[int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:            \n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
    "                else:\n",
    "                    p_data_fp32.add_(-step_size, exp_avg)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss\n",
    "\n",
    "class PlainRAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "\n",
    "        super(PlainRAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(PlainRAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                beta2_t = beta2 ** state['step']\n",
    "                N_sma_max = 2 / (1 - beta2) - 1\n",
    "                N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:                    \n",
    "                    step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
    "                else:\n",
    "                    step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
    "                    p_data_fp32.add_(-step_size, exp_avg)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class AdamW(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, warmup = 0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay, warmup = warmup)\n",
    "        super(AdamW, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(AdamW, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                \n",
    "                if group['warmup'] > state['step']:\n",
    "                    scheduled_lr = 1e-8 + state['step'] * group['lr'] / group['warmup']\n",
    "                else:\n",
    "                    scheduled_lr = group['lr']\n",
    "\n",
    "                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
    "                \n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * scheduled_lr, p_data_fp32)\n",
    "\n",
    "                p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 128\n",
    "sz = 128\n",
    "\n",
    "seed = 12345\n",
    "\n",
    "tfms = get_transforms(do_flip=True, flip_vert=True, max_rotate=15, max_zoom=1.15, p_lighting=0)\n",
    "\n",
    "data = (ImageList.from_df(path=PATH, \n",
    "                          df=df, \n",
    "                          folder='train',\n",
    "                          cols='objID',\n",
    "                          suffix='.jpg')\n",
    "        .split_by_rand_pct(0.2, seed=seed)\n",
    "        .label_from_df(cols='oh_p50', label_cls=FloatList)\n",
    "        .transform(tfms, size=sz)\n",
    "        .databunch(bs=bs)\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(data, base_arch=models.resnet34, opt_func=partial(RAdam), pretrained=True, ps=[0.25, 0.5], loss_func=root_mean_squared_error)\n",
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 48:49 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8.736033</td>\n",
       "      <td>8.618010</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.610697</td>\n",
       "      <td>7.322693</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.763615</td>\n",
       "      <td>0.360873</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.187478</td>\n",
       "      <td>0.317582</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.948650</td>\n",
       "      <td>0.287997</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.786413</td>\n",
       "      <td>0.294814</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.682578</td>\n",
       "      <td>0.175515</td>\n",
       "      <td>01:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.594320</td>\n",
       "      <td>0.249324</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.544260</td>\n",
       "      <td>0.124540</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.497136</td>\n",
       "      <td>0.207302</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.464251</td>\n",
       "      <td>0.130657</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.434055</td>\n",
       "      <td>0.133503</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.414138</td>\n",
       "      <td>0.212522</td>\n",
       "      <td>01:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.396604</td>\n",
       "      <td>0.122661</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.379036</td>\n",
       "      <td>0.262580</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.370052</td>\n",
       "      <td>0.105599</td>\n",
       "      <td>01:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.360683</td>\n",
       "      <td>0.219117</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.355521</td>\n",
       "      <td>0.103172</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.352868</td>\n",
       "      <td>0.208626</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.346235</td>\n",
       "      <td>0.304787</td>\n",
       "      <td>01:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.341498</td>\n",
       "      <td>0.230071</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.337058</td>\n",
       "      <td>0.224745</td>\n",
       "      <td>01:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.337680</td>\n",
       "      <td>0.122694</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.334006</td>\n",
       "      <td>0.129242</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.330716</td>\n",
       "      <td>0.220861</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.324153</td>\n",
       "      <td>0.144524</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.325082</td>\n",
       "      <td>0.129215</td>\n",
       "      <td>01:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.328783</td>\n",
       "      <td>0.102365</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.323407</td>\n",
       "      <td>0.096977</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.319940</td>\n",
       "      <td>0.094750</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.323184</td>\n",
       "      <td>0.098247</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.318802</td>\n",
       "      <td>0.093745</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.316468</td>\n",
       "      <td>0.106198</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.313233</td>\n",
       "      <td>0.095499</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.315155</td>\n",
       "      <td>0.091199</td>\n",
       "      <td>01:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.314106</td>\n",
       "      <td>0.093672</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.314122</td>\n",
       "      <td>0.090953</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.314930</td>\n",
       "      <td>0.090662</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.316730</td>\n",
       "      <td>0.090610</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.316095</td>\n",
       "      <td>0.090945</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(40, slice(1e-2,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like slow progress and didn't perform as well as regular Adam. Clearly something else needs to be adjusted (perhaps a strong warmup phase is still needed), but the final result is still promising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEKCAYAAAAxXHOuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJztnXt8XHWd99/fJG3SJmmatCm0tNC0cmuhtCEUXbkKy03lJkqrPCKiCMrjrjz7PLK6j7Dswy6urosXVkWhqKtUhAURQVCsi6hAWy4FikgLpaTpjaRt0ia9JPk+f/zOmZxMzsycSc5kLvm+X6+8ZubMOWd+Z6adz3y/39/38xNVxTAMwzDioizfAzAMwzBKCxMWwzAMI1ZMWAzDMIxYMWExDMMwYsWExTAMw4gVExbDMAwjVnIqLCJyjoi8KiLrROT6kOevFpEXReR5EXlSROYFnvt777hXReTsXI7TMAzDiA/JVR+LiJQDfwH+GmgFVgJLVXVtYJ9Jqtrp3T8f+LSqnuMJzN3AYmAG8BvgCFXty8lgDcMwjNjIZcSyGFinqq+r6n5gOXBBcAdfVDyqAV/lLgCWq+o+VX0DWOedzzAMwyhwKnJ47kOAtwKPW4ETk3cSkc8A1wHjgfcEjn0q6dhDQo69CrgKoLq6+vijjjoqloHnjb79sPVlmDQDag7K92iMONjbCR3r3f2D5kP5+PyOxzCSWL169duq2hjnOXMpLBKybUjeTVVvA24TkQ8D/wBcnsWxtwO3A7S0tOiqVatGNOCC4I6zYF8XfPpP+R6JEQcr74BfXufuX34bNJ2c3/EYRhIi8mbc58xlKqwVmBV4PBNoS7P/cuDCYR5bOhz7Qdi2Fra8lO+RGHHQGfhn2/F6/sZhGKNILoVlJXC4iDSJyHhgCfBgcAcROTzw8L3Aa979B4ElIlIpIk3A4cAzORxr4TD/YiirgBfvSb/fG7+HP3wdzES0sOlsg9rpLgVmwmKMEXKWClPVXhG5FngUKAfuVNWXReQmYJWqPghcKyJnAgeAHbg0GN5+9wBrgV7gM2NmRlj1FJh7Brx4H5xxI5Qlaf/eXfDrL8Hqu9zjYy6BuiHlJ6NQ6NwEdbOgstaExRgz5LLGgqo+DDyctO1Lgft/k+bYm4Gbcze6AmbBh+C+K2HjH2H2SQPbX/0VPPQ52L0FZp8MG34Pe7absBQynW2uaD+xATreyPdoioYDBw7Q2trK3r178z2UkqGqqoqZM2cybty4nL9WToXFGCZHngvjquHFnzlh2fM2PPJ5eOlemDYflvwn9PXCnWc5YTEKE1UnLIefBahLX6qChM1NMYK0trZSW1vL7NmzEXu/Royq0t7eTmtrK01NTTl/PROWQmR8NRz1Xnj5AZj1Tnjsi27a6mlfgJM+BxWBfL0JS+Gydxcc2OOmj1dUuvu7t0GtTSXPxN69e01UYkREmDJlCtu3j873hXmFFSoLPgR7d8IDV0N9E1z9ezjt805UAKq9aee7t+VvjEZ6/Blhk2ZAg/cr0eoskTFRiZfRfD8tYilU5pzuph7PaIYTPwVl5YOfH18DFRMsYilkEsJyCNR4PwQ6XofD3pW/MRnGKGDCUqiUV8AHvp/6eRH3ZWXCUrh0bnK3k2a4KcdlFRaxFAnt7e2cccYZAGzZsoXy8nIaG92Pg2eeeYbx4zM7KFxxxRVcf/31HHnkkTkdayFiwlLMVJuwFDSdbYBA7cHuh8LkQ01YioQpU6bw/PPPA3DjjTdSU1PD3/3d3w3aR1VRVcqSWwI8li1blvNxFipWYylmqqfBbhOWgqVzk/N8K/emdzbMMWEpctatW8cxxxzD1VdfTXNzM5s3b+aqq66ipaWF+fPnc9NNNyX2Pemkk3j++efp7e1l8uTJXH/99Rx33HG8613vYtu20q6NWsRSzFRPhbbn8j0KIxWdbTBp+sDjhjnw1kqbcpwl//iLl1nb1pl5xyyYN2MSN7x//rCOXbt2LcuWLeM73/kOALfccgsNDQ309vZy+umnc8kllzBv3rxBx+zatYtTTz2VW265heuuu44777yT668fskRVyWARSzFTM82lwvr78z0SI4zONle492mYA/t2QXdH/sZkjJi5c+dywgknJB7ffffdNDc309zczCuvvMLatWuHHDNhwgTOPfdcAI4//ng2bNgwWsPNCxaxFDPVjaB90LPDWcEYhUVn22A344Y57rbjdfu8smC4kUWuqK6uTtx/7bXX+PrXv84zzzzD5MmTueyyy0LdAoLF/vLycnp7e0dlrPnCIpZixu9lsQJ+4bGvy0Unk2YMbAsKi1ESdHZ2Ultby6RJk9i8eTOPPvpovodUEFjEUswkhGUbUOSLnJUanZvdbTAVNvlQkDITlhKiubmZefPmccwxxzBnzhze/e5353tIBYEJSzFTM83dWsRSeAR7WHwqKqFupglLkXHjjTcm7r/jHe9ITEMG183+ox/9KPS4J598MnF/586diftLlixhyZIl8Q+0gLBUWDGTsHUxYSk4gnYuQWzKsTEGMGEpZiY0uNSKRSyFR5cnLLUmLMbYw4SlmCkrg4lTvRqLUVB0tsHEKTCuavD2hjnQ0+Fm8hlGiWLCUuzUTHPrtRiFRWfb0DQYBGaG2aJfRuliwlLsVE816/xCpHPT4BlhPvVmn2+UPiYsxU71NKuxFCKpIpb62e7WIhajhDFhKXbM4bjwOLAXutvDhWX8RFfQt4iloDnttNOGNDveeuutfPrTn055TE1NDQBtbW1ccsklKc+7atWqtK9966230t3dnXh83nnnDZquXAyYsBQ7NY1woBv278n3SPLPrk2F8T50BRb4CsNmhhU8S5cuZfny5YO2LV++nKVLl2Y8dsaMGdx7773Dfu1kYXn44YeZPHnysM+XD0xYip1qr0nS6izwvffAk7fmexSpe1h8GppMWAqcSy65hIceeoh9+/YBsGHDBtra2li4cCFnnHEGzc3NHHvssfz85z8fcuyGDRs45phjAOjp6WHJkiUsWLCASy+9lJ6ensR+11xzTcJu/4YbbgDgG9/4Bm1tbZx++umcfvrpAMyePZu333YTdL72ta9xzDHHcMwxx3DrrbcmXu/oo4/mk5/8JPPnz+ess84a9Dr5wDrvi52ErcvbA+uqj0V698HuLbCrNd8jGbwkcRgNc9wU8X1dUFk7euMqVh65Hra8GO85Dz4Wzr0l5dNTpkxh8eLF/OpXv+KCCy5g+fLlXHrppUyYMIH777+fSZMm8fbbb/POd76T888/P+V68t/+9reZOHEia9asYc2aNTQ3Nyeeu/nmm2loaKCvr48zzjiDNWvW8NnPfpavfe1rrFixgqlTpw461+rVq1m2bBlPP/00qsqJJ57IqaeeSn19Pa+99hp333033/ve9/jQhz7Efffdx2WXXRbPezUMLGIpdmqCfmFjGN+Kfm8B5KJ9O5fa6eHP25TjoiCYDvPTYKrKF77wBRYsWMCZZ57Jpk2b2Lp1a8pzPPHEE4kv+AULFrBgwYLEc/fccw/Nzc0sWrSIl19+OdRuP8iTTz7JRRddRHV1NTU1NVx88cX8/ve/B6CpqYmFCxcChWHLbxFLsWMOx44eT1h6CkFY2qCqDiprwp8PuhxPXxC+jzFAmsgil1x44YVcd911PPvss/T09NDc3Mxdd93F9u3bWb16NePGjWP27NmhNvlBwqKZN954g69+9ausXLmS+vp6Pvaxj2U8j6qmfK6ysjJxv7y8PO+pMItYih3zC3P4EUshdLQnL/CVTIP1shQDNTU1nHbaaXz84x9PFO137drFtGnTGDduHCtWrODNN99Me45TTjmFH//4xwC89NJLrFmzBnB2+9XV1dTV1bF161YeeeSRxDG1tbV0dXWFnuuBBx6gu7ubPXv2cP/993PyyScP2a8QsIil2KmohMo6i1h6CiwVlqpwD66uUj3NhKUIWLp0KRdffHEiJfaRj3yE97///bS0tLBw4UKOOir9chXXXHMNV1xxBQsWLGDhwoUsXrwYgOOOO45FixYxf/78IXb7V111Feeeey7Tp09nxYoVie3Nzc187GMfS5zjE5/4BIsWLcp72isMSRdejfjkIucAXwfKge+r6i1Jz18HfALoBbYDH1fVN73n+gC/YrdRVc9P91otLS2aaX54yfLN410x8oN35Xsk+WPVnfDQ56BiAvzDlvyO5atHwBFnw/nfTL3PHWdDWQVc8cvRG1cR8corr3D00UfnexglR9j7KiKrVbUlztfJWSpMRMqB24BzgXnAUhGZl7Tbc0CLqi4A7gX+NfBcj6ou9P7SisqYp7rR/ML8VFhvj5shli9697up38muxslYL4tRwuSyxrIYWKeqr6vqfmA5cEFwB1Vdoap+J9BTwMwcjqd0qW60PpZgbSWfBfzdWwBNnwoDJyxdbbC/O/1+hlGE5FJYDgHeCjxu9bal4krgkcDjKhFZJSJPiciFuRhgyZCtrcuBHnjs/8LeXbkb02jjRyyQ3zpLph4WH7+Av2NDTodTzOQyTT8WGc33M5fCEtYxFHplInIZ0AJ8JbD5UC/v92HgVhGZG3LcVZ74rNq+fQwXr2umueJ134Fo+7/5B/jjN2D9isz7Fgs9AWHJZ8QStiRxGMEpx8YQqqqqaG9vN3GJCVWlvb2dqqqqzDvHQC5nhbUCswKPZwJtyTuJyJnAF4FTVTWRHFfVNu/2dRH5HbAIWB88VlVvB24HV7yPefzFQ7XXodvdDrUHZ96/3fsy6y6hukx3O1ROgn2dBRKxZBIWm3KcjpkzZ9La2sqY/sEYM1VVVcycOTrVhlwKy0rgcBFpAjYBS3DRRwIRWQR8FzhHVbcFttcD3aq6T0SmAu9mcGHfCBL0C4siLP6XWSkV/Ls73Jf15hfy28vS2Qbjql2DZDom1LulpU1YQhk3bhxNTWPYoqjIyVkqTFV7gWuBR4FXgHtU9WURuUlE/FleXwFqgJ+JyPMi8qC3/WhglYi8AKwAblHV9H4HY5nqLG1dOrzAr5R6X3o6oMHLluY1Featw5LCO2oQNjPMKFFy2iCpqg8DDydt+1Lg/pkpjvsjcGwux1ZS1HgRS9QIpNQilv4+JyZ+einfqbBMaTCfhjmw8ancjscw8oBZupQCfo0lypTjvt6BmUilIiw9OwF1KcHxtQUQsWSYEebTMAd2vZXfvhvDyAEmLKVA5SQor4yW2tr1FvT3uvulUrz3Z4RNbIAJk/MXsfT3QdfmLCKWJkBhR3q/KcMoNkxYSgERlw6LIix+GqzxqNKpsfg9LBMaoGpy/iKW3dtA+7JLhYHVWYySw4SlVKiemp2wzFrsvpD7+3I7rtGgUCKWqM2RPiYsRoliwlIqVE+LVmPpeB3GTYRp8wEd3LFerHQHhKWqLn8RS9TmSJ+JU1wa04TFKDFMWEqFqEaU7evdL2V/5clSqLN0t7vbCUUWsYi4OosJi1FimLCUCjWeX1gmC4yO152wTPRmkpVCnaWnw1nQV9Z6NZY8NUh2bnKTKCY2RD/GelmMEsSEpVSoboT+A+l/rff3uanGDXMCTZWlELF0uLSSiOto790LB9Iv85oTsmmO9GmYAzs3Rvd5M4wiwISlVEjYuqSJQHa95cRnytyB3pdSEJaeDpcGA5cKg/ykw7LpYfFpmONmkr36cOZo0zCKBBOWUqE6QmrLT7k0zPG+iKVEaiwdA+mnKk9Y8lHAz7QkcRhzz4C6WXDPR2HZufDGE7kZm2GMIiYspULC1iXNzLB2zyOsYQ6UV7gv41KosXR3uBQY5C9i6e/PrjnSZ9J0+J+r4byvujTlD94Pd70PNvwhJ8M0jNHAhKVUiFIz6XjDrQlfO909nji1dFJhiYjFE5jRjli626Fvf/apMICKSlj8Sfjs83DOl+Htv8Bd58EPzoeNT8c/VsPIMSYspcLEKYCk72Xp8KYa+8XlqFOUCxnVgeI95C9iSfSwTB/+OcZVwTuvhr95Ac7+Z9i2Fu48Cx79YjxjNIxRwoSlVCgrd1+umWosU+YMPK7OsH8xsH+3m5AwIc81lqgLfEVh3AR412ecwMw5Hdb+fOTnNIxRxISllEjnFxacauxT3Vj8xXu/OTKRCvMW2BrtXpZExDKMVFgqxlfDjEWudtPfH995DSPHmLCUEun8wna1uhpAUFgmTnVfwMXcQxE0oAQ3KWF8bR5SYW2uSdOvdcVF7XTnRu0LaJz85FJY8c/xn9cY85iwlBLp/MISU43nBvb3pigXs19Y0IDSZ0J99qmw534MK+8Y/jg625wIlJUP/xxh+EtNd22O97yq8Mbv3Z9hxIwJSymRrhjfEZhqnNi/BGxdur2Ul1+8B5hQl33EsurOEQrLMHpYouCfM25h6dkBB/bAjjfiPa9hYMJSWtQ0wv4uONAz9LmON6CiamCqMQykbYq5ztKTlAqD4a3Jsmcb7N46/HEMp4clCrmKWPyaUNdm2N8d77mNMY8JSymR6GUJiUB888mywEc+sQRsXbrbARmYZgzZOxyruhRid7tbujlbVIdn5xKFmoMAga4t8Z5316aB+/5S1YYREyYspUQ6vzDfLn/Q/iVgRNnd4WaCBWsb2UYs+zqdcSU6vCL53p1woDs3EUv5OPc5+dOZ42LXWwP3LR1mxIwJSylRkyJi6e9zXx7JwjKhHqSscGose9rh6wuh7fnox/R0DK6vQPYRS1CI01nipCLOHpYwag+OP2Lp3AR4jbIdJixGvJiwlBKJCCTpy7Fz09CpxuDSYhOnFE6NZcsLTgDfysLGJGhA6VM12bPOD6k1hRGsrURZhTOZbBf4ypba6fHXWHZtcuaXVZMtYjFix4SllPCFJfnLMehqHHZMoaTC/Fx/ME2Tie72wYV7GKi3RE2HjVhYslySOFsm5UJYWqHuEG8FSxMWI15MWEqJcRNcc2CyUPjCMmXu0GMmTsleWPa056apMiEsrdGP6dkxNGLxnY6jpsP2xJEKE6/QngNqp7sxxvmed7ZC3Uyot6WRjfgxYSk1ahqHfjm2r/emGof8oq5uzK7G0t8H32qBp/5jZOMMYzjC0t0xNGLJ1i9s91aQcvceDSdi6drs7HTKx2V/bBT8KccjmQ4dpL8POje71F1Dk4sQhzMbzjBSYMJSaoQJRccb7pdpWcjHXT01uxpL5yZXMN+8ZmTjDCNbYend55r8hkQsWToc797qhCGd11o6urbmLlqBgR8EnTGlw3Zvc8adfsTS35td+tEwMmDCUmpUNw6dbtwRMtU4uP/eXdC7P9r5d250t7ko+PrC0rUl2ni6Q+xcYBgRy3b3PqSzxElH1+bBjadxE3eTpF8Tqps58O/CCvhGjORUWETkHBF5VUTWicj1Ic9fJyJrRWSNiDwuIocFnrtcRF7z/i7P5ThLiuSIpb/fRSwNTeH7+1N1o0YtO950t3EXfHt2OIFrPBpQ6IrQt+H3nAwp3mdZY9ntRRzDjVh2b4XaXEYsnmjFJSx+ROinwsDqLEas5ExYRKQcuA04F5gHLBWReUm7PQe0qOoC4F7gX71jG4AbgBOBxcANIlKfq7GWFDXTBneQd26Cvn3hhXvIvknSj1h6OpwQxIUfrcw+yd0GO8NTEWZACQHr/KjCss0JS3Vj9nWMvl7v+IOzOy4bJk6BsnHxC0vdTDfuiiqbGWbESi4jlsXAOlV9XVX3A8uBC4I7qOoKVfWNip4CZnr3zwZ+raodqroD+DVwTg7HWjpUNwI68KWbbqoxZG9EufPNgftxfhkNEZYIdZZky3yfsnKonBQtYunvd9de0+jEpbvdFbejsmc7oAPpqlxQVhZvk2TnJhg30UV2ZWVQP9tsXYxYyaWwHAIEK4Kt3rZUXAk8ks2xInKViKwSkVXbtxdI93i+Se5lCbPLD9s/qpXJzo0DEUGceXn/i+2wd7vbKMXkRMQyZehzVZOjLfa1d6crZPupMO3PztbFjyJyKSz++eOyddnV6tJg/hLV9dbLYsRLLoVFQrZp6I4ilwEtwFeyOVZVb1fVFlVtaWyMeYGlYiXZiLJjPZRXpu4K97+Uo0YsO96E2Sd7545ZWCZOdZHDxCkRI5ak1SODTKiLlgrzU18101I3mEY5PufCMj2+iGWX18Pi0zDH/UjQ0P+ehpE1uRSWVmBW4PFMYMhPLhE5E/gicL6q7svmWCOEGs+IMiEsb7hUR9hUY3C/7MsqotVYeve7ovq0ee5LOO6IpX62u183M6Kw7IBx1VBROfS5CfXRUmG+iPgRC2TXJOlHLLmssUC8wtK5yXXd+zQ0ORPNuPpkjDFPLoVlJXC4iDSJyHhgCfBgcAcRWQR8Fycqwf/NjwJniUi9V7Q/y9tmZCK5ZtK+PnXhHjy/sDRLGgfpbHWpovrD4k+fDBKWWdGEpSfEJ8wnqsOxLyzV0wLu0NkIy1Zc1/206McMh9qDYd8u2L9nZOfp3e+ub1IgYqn3Z4ZZOsyIh5wJi6r2AtfiBOEV4B5VfVlEbhKR873dvgLUAD8TkedF5EHv2A7gn3DitBK4ydtmZKJqsptBtHubK0yHuRonUz01Wl3BnxE2+VD3Kzeugm9fL+x8KylieStzaibMgNInqsNxMBVWMxxh2ezev1x13fskVpIcYdTS1QZoUirMExbrZTFioiKXJ1fVh4GHk7Z9KXD/zDTH3gncmbvRlSgiA8aSXZudy28UYYkSsfg9LJO9iGXNPa77PSwVlQ2draB9g4Vl/243nTm4gFcyPSF2Lj5RI5Y921wNyp+QUFGVXSps99bc11dgcJNkugg0E4mpxoFUWN0st3yCRSxGTFjnfSni+4WFrXMfxsSp0WosOzc6T61EY50OiM1I8COfoLBA5nRYd3v6iKVvX2br/N3bXKQi4onytPCF0lLRtTn39RUYaJIcqa2L3x8UTIVVjHfiYhGLERMmLKVItddBnqmHJbF/ROv8nRvdL93yioG8fBxfRkOExZu3kVFYMkQskDlq8X3CfMJMPNPRNVoRS0zd950hEQt49vnWfW/EgwlLKeL7hbWvh/Lxg/PpoftPgf1dcGBv+v12vunSYBCwAolJWMrGDdQR/PF2phGW/j6XKksXsUDmXpbd2wcbSGYTsfT3OREaDWGprHUz4EZaY9nV6mbMja8evN16WYwYMWEpRWoaByKW+tmD14MPI9EkmSFq2blxQFiqG90XXVwRy+RDB8ZZPc0JTbqIpWcnoOHNkTAQsWQq4O/eOnD94N67qNNu92x3s+RGQ1hEvO77Ec6637VpcBrMp6EpfpseY8xiwlKKVDe6+sLmF1J33AeZ6E9RTiMsB/a6NMzkQ91jkfhWHwxONQY3BbrukAzCksLOxcc3okyXCuvvc2IajFhqDnLboti6jFYPi8+kGSOPWDo3hUewNuXYiBETllLE78fY9Vbm+gpEM6L0v+TrDxvYVj87voglKCzgflWnE5ZE130Kb9Ioa7J0t7uII1hjqfZtXSLMbu/yu+5zaJkfpPbgkddYdr01tL4CZp9vxIoJSyniN0lCarv8sP3TTTneucHd+hGLf+4db7p+meHSs9PVQZKFJVP3fSoDSp8oxftgD4tPjS+yEQr4CZ+wHFrmB6k92M0KG671yj5vCneYvY///lsB34gBE5ZSJPhFGaXnwReWdDWWRHNkMGJpcim3keT9fbfkMGHpbEu9ZG4qy3wfvy8lXcSSEJak4j1Ea5IMOz6X1M5w73cUc80wEgt8zRr6XGWNu3ZLhRkxYMJSigSL0VFSYZWTXLE8XcSy4023T7BQHcfMsOSpxj51M13T5O4UNYXuNM7G4FnnZzCi9Gd/DYpYshCWrs2uPpXrrnufRJPkMOssvmN0WCoM4nVTMMY0JiyliF+MLxsX/us0mUS3fhpbl50bYfKswTPM4sjLJ4TlsMHbM/WydLe76xtfk/rcE+qiRSzVIcISKRW2dfTqKxDoZRlmhJhojkwhLDbl2IgJE5ZSpLzC1R6iTDX2yWTrsvPNwfUVcAX2soqRRywTGgZSVz6Zuu99A0p/TZEwMtm67N7mpkxXBsSpcpKzeIkasYxWfQVgki8sw4xYOjcBMtAvlEzDHLdPpn4mw8iACUupMvlQmHZU9P2rp2ausSQLS3mF2zbSiCU5DQYD6ZpUC36l67r3mZBhsa8924a6EovnVBzFO220fMJ8agJ+YcNhV6sbb6rUnW/TszMGmx5jTJNTE0ojj3zoB1AxIfr+1Y3Qvi78uf173Bft5MOGPjfS9MmODTB94dDtlbUu4vDTN8n07EhdX/Gpmgxdf079fLKdi091Y+aIpb/PO34UhWVclevPGa5fmL9yZCqCvSyNRw7vNQwDi1hKl/rZ2aVpJk5NXWPZ6UUNYcLS0DT8iKW/z0VCYRELpF+XpbsjdQ+Lz4T6zKmwMGGpmZa5xrLn7dHrug9SO4ImyVTNkT5mn2/EhAmL4aieCgf2wP7uoc8lpgSniFj27orWUJhM5ybo700jLGl6Wbrbo6XC9u5M3fexe2v4VOEoEctorXWfzHCbJFWHLkmczMQpML7WCvjGiDFhMRzpelmCC3wlM5JfuammGvvUHRJeY1FNv3qkT9Vk6Nsfbp3fu9+l06rDIpaDXESSrvFz9yh33fvUTh+esHR3uLV50qXCEjY91iRpjAwTFsORsHUJKVrv2OAWwAr7dT8Sj6mMwjLTRRz7ugZv39flIp0oEQuETzneE9LD4lMzzfXQ9KSJwhI+YaM4KwzczLDdW6N5mQVJ2OVncLoeSWrTMDxMWAxHwogypM6yc6O3ymDI1N6EFcgwhaWsIvWv6EQvS1IBvydDc6RPOluXdF3zvsimS4d1jXLXvU/twa62E2XWWpCwlSPDqPdterIULsMIYMJiONL5he18M7y+AjB+opsZNdxUWN0sN205jFS9LAkDyhxGLJC+gN+12Qlbxfj0Y4ibxEqSWTZJhq0cGUZDE/QfGLB/MYxhYMJiONIKS0gPS5Dh2uen6mHxSQhLUp2l2+tNyZQKS0QsIb0sYQaUPlH8wnaPcte9T+0wmyQ7W92ib0G7nzDMPt+IARMWwzG+xtVRkov3ezvdF3PYVGOf+mHm5TMJS83BIOVDI5ZMBpQ+EyKkwkKL9xGEpWvz6KfBYPhLFO9qdR33ZRnF0Hx8AAAegUlEQVT+y/s2PVbAN0aACYvhSPiFJQlLuhlhPg1N7osubPZVKvZ2upRWOmEpr3BfhslpmUyW+T7pVpHcvd2ZVI6rCjmuzv26T5sKy1PEUt0IUjYMYUmxcmQyk2a4a7cCvjECIgmLiMwVkUrv/mki8lkRmZzboRmjzsQpqYUlVY0FBtIn2TjjprLLTyasl6WnA5CBiCQVVXVuv1QRS1gaDDyRnTbgfpyM33U/mj5hPuUVLlLKVlgyNUf6lJW76NRSYcYIiBqx3Af0icg7gDuAJuAnORuVkR+qG4fWWHwBSJcKG459fqapxj51M0NqLO1OVDIZbJaVQ9WkFBHLtvSprJrG1BFLd7ubjpyPiAUGFvyKSn+fK/ZnmhHmY1OOjRESVVj6VbUXuAi4VVU/B+Tpf5WRM6qnDsy48tm50TkAp5vaWz+MJsmshGXT4GbFKAaUPqkcjsMMKINUT0tdY8lXD4tP7fTsivddW5wQpmuODFLfBB0bhr9SpTHmiSosB0RkKXA58JC3bZRWNzJGDd86P/iFssOzy09nTz+xwdnNZxuxVE3OnM6adIib/hqMHqJ03fv4ti7JpPIJ86lJY+sy2mvdJ5Nt9326lSPDaJgD+7uGpkUNIyJRheUK4F3Azar6hog0Af+Zu2EZeWHiVGf7sX/PwLadG9PXV8CJTv3s7COWTNEKhC/4NdKI5UAP7OvMICwHOZENs3XxV7XMR40FnLD0dERfNyXTypHJmBmlMUIiCYuqrlXVz6rq3SJSD9Sq6i2ZjhORc0TkVRFZJyLXhzx/iog8KyK9InJJ0nN9IvK89/dg5Csyhk+yrYtq+AJfYTTMyT5iiSQsIb0sUSzzfcLWZPEjkXSprGrf1iWkB8ZPQ+UtFeYZX6ZatjmZTCtHJmO9LMYIiTor7HciMklEGoAXgGUi8rUMx5QDtwHnAvOApSIyL2m3jcDHCJ8I0KOqC72/86OM0xghCSNKr86yd6f7ZZ+ucO/T0OSimyhWIJns8oOEdd93t0dPhVWFpMJ8YQnrYfGp8UU2JB3WtcVFTBWV0cYQN9muJNm5ybkWJ6/SmYr6wwCxiMUYNlFTYXWq2glcDCxT1eOBMzMcsxhYp6qvq+p+YDlwQXAHVd2gqmuANDayxqiR3H2/w58RFiFiqfesQFLZ3Afp2uxch6MIS1Wd+1L0z3tgLxzodmutRGGClwoL1o3Sdd37pOu+79qSv/oKZG/rsqvVpcHS1cmCVFS66MaaJI1hElVYKkRkOvAhBor3mTgECM4TbfW2RaVKRFaJyFMicmHYDiJylbfPqu3bszTlM4aSMKL0irZRelh8ssnLR50RBu7LMNjLErXr3mdCvRO8A4F1ZvZESIWl677fvSV/9RXI3tYl0zosYQzXpscwiC4sNwGPAutVdaWIzAFey3BM2M+jbOYvHqqqLcCHgVtFZO6Qk6nerqotqtrS2JjBA8nITHLEsjPLiAWifRllIywwWFi6Izob+4Q5HCdSYVNTH5fOiDLfEcuEeiivjD4zrHNT9PqKj/WyGCMgavH+Z6q6QFWv8R6/rqofyHBYKxCc3zgTiGzJqqpt/msBvwMWRT3WGCbjq2HcxIEay86NbhpxVQSTBd8KJEr6ZMcG5wEW9Vf0IGHxxhZ1VliYw/HurU6YytPMmK+a7K4nOWLp70+98uRoIRJ9JckDe90PhWwjlvomd1zyWjiGEYGoxfuZInK/iGwTka0icp+IZPqXuhI4XESaRGQ8sASINLtLROoDFjJTgXcDa6Mca4wQv5cFvB6Ww6Ll5n0rkKipsLqZ6b/Yg9TNdOaYB3qyT4WlilgyCUPCOy0pxdrd7hYZG+0liZOJ2iSZ6GEZRioMoH1ddscZBtFTYctwojADVyf5hbctJV6n/rW4FNorwD2q+rKI3CQi5wOIyAki0gp8EPiuiLzsHX40sEpEXgBWALeoqgnLaBD8Ms1kl59Mg9exnYmoU419EjPDNkU3oPQJjVi2ZbaPB7dPcsSS6GHJs7BMitgk2ZnlVGOfWSe66PXxf7IOfCNrogpLo6ouU9Ve7+8uIOP/TFV9WFWPUNW5qnqzt+1Lqvqgd3+lqs5U1WpVnaKq873tf1TVY1X1OO/2jmFen5EtE6e64r3fwxKlcO/j2+dn+iIatrC8FVPEEjGVVTNtaI0l0cNSABFL5+bM7/WuiEsSJzNpBpz1T7D+cVh15/DGaIxZogrL2yJymYiUe3+XASFr2BpFj2+d393uZlJlG7Hs353eCmTfbhcRDUtYWt0iX+NroveQJNZk8RodVd3rp5tq7BPmF9ZVIBFL7cFwYE/mGkiiOXJG9q/RciXMfQ889g/Qvj77440xS1Rh+ThuqvEWYDNwCc7mxSg1qqe4ekaihyXLiAXS11mi2uUHqZ0BiCcs7dHTYODWXEEGUmH7dzvBjCIsNdOG2rrku+vep9YTikx1ls5WF4WOm5D9a4jA+d9ytbAHronW/GoYRJ8VtlFVz1fVRlWdpqoX4poljVKjutE1L2590T3ONmKB9FOOs51qDG5d+dqDnbD0dMDEiM2R4FZMrJo0kAqLYufiUzPNFeoH1We2uOm+YQuEjSZ+xNSVYaKl3xw5XOoOgfO+Cm89DX/8xvDPY4wpRrKC5HWxjcIoHPwmyU2r3W02wjI5ghXIcIQFBtZlycaA0mdC/YA4JIQlSiqscfAx4CKEfNdXIHqT5K5N0V2NU3HsB2HeBfDbm2HLSyM7lzEmGImwRPSHMIoK/8t007PuC7lqUvRjx1W5XH6miKWyLroli4/fy5KNZb5P0OE43Vr3yYQ1SXZtyX99BQIRS4aZYcNpjkxGBN777+4zu/9T0LtvZOczSp6RCIvNQSxFqr2O9m2vZFdf8alP0bG9px1eug/Wr3AzzaL6VvnUzXRfknvao3fd+wTXZMkmFRbmF1YowlJZ45pX060kuXeXMxEdSSrMp3oKnP8N2PoS/C6jsbkxxqlI96SIdBEuIAIMoxpoFDx+xKJ92aXBfBpmw18ec79qNz4Fr6+A9b+FzWsAdaaSpw5ZQSEzdbPcWjG9e7NPhVVNHpgdtWeb6/qPEvUk+4X5XfeFICyQuft+1zCbI1Nx5Lmw6DL4w63u/qzF8ZzXKDnSCouq1o7WQIwCYWLAPyubHpbEMU3uy/uWw6C3B8oqXLPd6V+EuafDjEWZ16oPI/jlmG0qbFDEstW5C0QZw4R6KBs3kArr6XCGloVQY4HM3feJ5siYhAXg7H+B159wKbGrn3Q2QIaRRFphMcYg46qcTf3+ruGlwg7/a/jLo05A5r4HZr8bKmP4fRKsEwwnYunZ4XpYMi1JHMS3ddntOREUSg+LT+10ePMPqZ/f/IK7jSMV5lM1CS78D/jB++A3N8J5X4nv3EbJYMJiDKV66vCFZfpx8Ilfxz+m4Mym4UQs/b1uyeUoPmFBahoHIpaCE5aD3Zj6+920ap+urfDoF+Cle93nEbcTc9PJcPwVsGoZnPr59C7RxphkJMV7o1TxvyiGU2PJFRMboGLCwP1sqAr4he3eFm1GmE+w+75QfMJ8Js1wqTnf5qa/39mv3HYCvPIgnPb38PHHhpd6zMSJn3Kv/cLd8Z/bKHpMWIyh+AX8QhIWf8EvGF4fC7h02J4sUmHg9vWFxS+UF0yNxRtHZxtsfRnuPBse+hwcvACu+SOcdn3uGjmnHQ0zF8OzPzSTSmMIJizGUCYf5kRl/MR8j2QwvrAMJxUGzqamb3+WqTDP1kXVpZiqJue/697HT3H95kb47inO4v7C78Dlv4Cph+f+9Zs/Cm//xXXlG0YAExZjKKd/Aa54JN+jGErdTDdLa3xNdsf5qbC3X3W32UQs1dO8dNMOF7EUShoMBoRl/eOwYAlcuwoWLs2+R2i4zL/IfRbP/nB0Xs8oGqx4bwylalJ2HfejxQmfgBkLs//i9COWt73VtLNNhYGLWgqphwWc0J51s3tPZp80+q9fWQPHfABe/Bmc8y+uR8kwsIjFKCZmLHTiki1+xLLdj1iySIUF/cIKxSfMRwT+6tr8iIpP8+XOLfql+/I3BqPgMGExSp/KSYC4egAML2LZvbVw7FwKiUOaYdp8S4cZgzBhMUqfsjKXptm/G8rHD0QwUfCjm+2vulqLCctgROD4y6HtOc+2xzBMWIyxgl9nqZ6WXY2marKzpdnifWmasAzl2A9CeSU896N8j8QoEExYjLGBH6VkkwYDF+1UN8IWb+GzQqqxFAoTG2De+bDmp3CgJ9+jMQoAExZjbOA3SWYrLOCExTd0tIglnOaPOpv+V36R75EYBYAJizE2mDDMiCX5GBOWcA47yTlbWxHfwITFGCskUmFZTDX28Y+pqoNxtgxRKGVl0Pw/YMPvoX19vkdj5BkTFmNsECzeZ4vfy2L1lfQc92G3iJoV8cc8JizG2GC4xfvgMZYGS8+k6XDE2fD8T6DvQL5HY+QRExZjbDBhBKmwahOWyDR/1DWTvvZYvkcylD3tcMfZ8Pt/y/dISh4TFmNsMP04t1jY1COyP7bGS4WZsGTmHX/tUoaFVsTv3Qc/vQzeegoevwmesJUvc0lOhUVEzhGRV0VknYhcH/L8KSLyrIj0isglSc9dLiKveX+X53KcxhhgxiL43EtQPSX7Y/2IxWosmSmvgEUfcRFLZ1u+R+NQhV/8LWz8I1z8fVhwKfz2/8GTt+Z7ZCVLzoRFRMqB24BzgXnAUhGZl7TbRuBjwE+Sjm0AbgBOBBYDN4hIfa7GahhpmfIOV5g+4ux8j6Q4WPQ/3Jf5jy6CtT93K1vmkyf/HV74CZz2BVjwQbjgP5wr829ugD9+K79jK1FyGbEsBtap6uuquh9YDlwQ3EFVN6jqGiD5X97ZwK9VtUNVdwC/Bs7J4VgNIzUV4+Gib8OUufkeSXHQ0ARLfgLaD/d8FG4/BV59JD8rTa59EB7/RzjmEjj1/7ht5RVw0e0w7wJ47Ivw1HdGf1wlTi6F5RDgrcDjVm9bbMeKyFUiskpEVm3fvn3YAzUMI2aOOg8+/RRc9F3YtxvuXgLfPwPW/WZkAuOv5BnlHG3PwX9dBTNPgAtuG+wRV14BH7gDjnof/OrzsPL7wx+TMYRcCkuY01/Uf1GRjlXV21W1RVVbGhsbsxqcYRg5pqwcjlsC166E87/p1rT5zw/AsnOdwET1FVN1zsmP3wTfbIZ/OwK+cxKs/gHs7w4/prMN7l4K1VNd9BS2nHT5OLhkGRxxLvzyf8Hqu4Z9qcZgcrmCZCswK/B4JhC1mtcKnJZ07O9iGZVhGKNL+Tg3DXnBpW622BNfdQJTVgEHzYdDWuCQ493f1CNcF7+qM/58+X5Y+wB0vO6aL5tOhuOWuhTXLz7r6iTNl7sF4CZ7Xzf797gIaV8XXPlY+t6livHwoR/A8o+4An9ZBSy6bHTelxJGNEd5TxGpAP4CnAFsAlYCH1bVl0P2vQt4SFXv9R43AKuBZm+XZ4HjVbUj1eu1tLToqlWrYr0GwzBywIEeWPc4bFoFm1bDpudgf5d7bnytWym0c9NgMZl/ERz1/oFZfarw5h/h6e/Anx9y2456Lyz+FDzzXfjzL2Hp8ugTLg7sdWL0+gqYuRhOuBLmXRge6ZQYIrJaVVtiPWeuhAVARM4DbgXKgTtV9WYRuQlYpaoPisgJwP1APbAX2KKq871jPw58wTvVzaq6LN1rmbAYRpHS3w/tr0GrJzRtzzqnhPkXDhaTVOx8C1bd4VJZPTvctrP/Bd716ezGcWCvq7WsuhM61sOEBjd1+vgrwiduqMKON+DNP8HGP7kIq/FIaDoFmk4diKAKnKITltHEhMUwxjgHeuDFnzmBWPzJ7BZ0C9LfD2/8txOYP/8StA/mvgdaroS6mbDxKdcTs/Ep5zIATggPPha2vQLdb7tt9U1OZOacCrNPGWi0LTBMWNJgwmIYRux0bnZ1odV3QVegRFx3KBz6Tvd32F/B1CMHakPbXnHC9MYTsOFJ2NfpHTPLuWOXV7rajn9bUeWWzC4rB8QTRAEpG7gP0LfPiWbvXuckELyddjQs+fGwLtGEJQ0mLIZh5Iy+XjeTbf9uJyZ1M6Mft+UFJzLb/uzEodf769vv3XqP+/sA9aZSe7faT2JCbEUVVFR6t1WDH089HE4bYm4SiVwISy5nhRmGYZQG5RVw5DB6tMsrBma8jSHMhNIwDMOIFRMWwzAMI1ZMWAzDMIxYMWExDMMwYsWExTAMw4gVExbDMAwjVkxYDMMwjFgxYTEMwzBixYTFMAzDiBUTFsMwDCNWTFgMwzCMWDFhMQzDMGLFhMUwDMOIFRMWwzAMI1ZMWAzDMIxYMWExDMMwYsWExTAMw4gVExbDMAwjVkxYDMMwjFgxYTEMwzBixYTFMAzDiBUTFsMwDCNWTFgMwzCMWDFhMQzDMGIlp8IiIueIyKsisk5Erg95vlJEfuo9/7SIzPa2zxaRHhF53vv7Ti7HaRiGYcRHRa5OLCLlwG3AXwOtwEoReVBV1wZ2uxLYoarvEJElwJeBS73n1qvqwlyNzzAMw8gNuYxYFgPrVPV1Vd0PLAcuSNrnAuAH3v17gTNERHI4JsMwDCPH5FJYDgHeCjxu9baF7qOqvcAuYIr3XJOIPCci/y0iJ+dwnIZhGEaM5CwVBoRFHhpxn83AoaraLiLHAw+IyHxV7Rx0sMhVwFUAhx56aAxDNgzDMEZKLiOWVmBW4PFMoC3VPiJSAdQBHaq6T1XbAVR1NbAeOCL5BVT1dlVtUdWWxsbGHFyCYRiGkS25FJaVwOEi0iQi44ElwINJ+zwIXO7dvwT4raqqiDR6xX9EZA5wOPB6DsdqGIZhxETOUmGq2isi1wKPAuXAnar6sojcBKxS1QeBO4Aficg6oAMnPgCnADeJSC/QB1ytqh25GqthGIYRH6KaXPYoTlpaWnTVqlX5HoZhGEZRISKrVbUlznNa571hGIYRKyYshmEYRqyYsBiGYRixYsJiGIZhxIoJi2EYhhErJiyGYRhGrJiwGIZhGLFiwmIYhmHEigmLYRiGESsmLIZhGEasmLAYhmEYsWLCYhiGYcSKCYthGIYRKyYshmEYRqyYsBiGYRixYsJiGIZhxIoJi2EYhhErJiyGYRhGrJiwGIZhGLFiwmIYhmHEigmLYRiGESsmLIZhGEasmLAYhmEYsWLCYhiGYcSKCYthGIYRKyYshmEYRqyYsBiGYRixYsJiGIZhxEpOhUVEzhGRV0VknYhcH/J8pYj81Hv+aRGZHXju773tr4rI2bkcp2EYhhEfORMWESkHbgPOBeYBS0VkXtJuVwI7VPUdwL8DX/aOnQcsAeYD5wD/4Z3PMAzDKHByGbEsBtap6uuquh9YDlyQtM8FwA+8+/cCZ4iIeNuXq+o+VX0DWOedzzAMwyhwKnJ47kOAtwKPW4ETU+2jqr0isguY4m1/KunYQ5JfQESuAq7yHu4WkVdHMN6pwNsjOL6YGcvXDmP7+sfytcPYvn7/2g+L+8S5FBYJ2aYR94lyLKp6O3B79kMbioisUtWWOM5VbIzla4exff1j+dphbF9/Lq89l6mwVmBW4PFMoC3VPiJSAdQBHRGPNQzDMAqQXArLSuBwEWkSkfG4YvyDSfs8CFzu3b8E+K2qqrd9iTdrrAk4HHgmh2M1DMMwYiJnqTCvZnIt8ChQDtypqi+LyE3AKlV9ELgD+JGIrMNFKku8Y18WkXuAtUAv8BlV7cvVWD1iSakVKWP52mFsX/9YvnYY29efs2sXFyAYhmEYRjxY571hGIYRKyYshmEYRqyMeWHJZDtTrIjIBhF5UUSeF5FV3rYGEfm1iLzm3dZ720VEvuG9B2tEpDlwnsu9/V8TkctTvV6+EZE7RWSbiLwU2Bbb9YrI8d77uc47NmxKfN5Icf03isgm79/A8yJyXuC5UMukVP8fvEk4T3vvy0+9CTkFgYjMEpEVIvKKiLwsIn/jbS/5zz/Ntef3s1fVMfuHm1SwHpgDjAdeAOble1wxXdsGYGrStn8FrvfuXw982bt/HvAIrn/oncDT3vYG4HXvtt67X5/va0txvacAzcBLubhe3KzEd3nHPAKcm+9rjnD9NwJ/F7LvPO/feiXQ5P0fKE/3/wG4B1ji3f8OcE2+rzlwPdOBZu9+LfAX7xpL/vNPc+15/ezHesQSxXamlAha6PwAuDCw/YfqeAqYLCLTgbOBX6tqh6ruAH6N824rOFT1CdzMwiCxXK/33CRV/ZO6/10/DJyrIEhx/alIZZkU+v/B+3X+HpztEgx+L/OOqm5W1We9+13AKzinjpL//NNceypG5bMf68ISZjuT7kMpJhR4TERWi7O+AThIVTeD+wcJTPO2p3ofiv39iet6D/HuJ28vBq710j13+qkgsr/+KcBOVe1N2l5wiHNIXwQ8zRj7/JOuHfL42Y91YYlkHVOkvFtVm3Hu0p8RkVPS7Dsia50iJNvrLdb34dvAXGAhsBn4N297SV6/iNQA9wF/q6qd6XYN2VbU1x9y7Xn97Me6sJSsdYyqtnm324D7caHuVi+sx7vd5u2e6n0o9vcnrutt9e4nby9oVHWrqvapaj/wPQYcwrO9/rdx6aKKpO0Fg4iMw32x/lhV/8vbPCY+/7Brz/dnP9aFJYrtTNEhItUiUuvfB84CXmKwhc7lwM+9+w8CH/Vmy7wT2OWlDh4FzhKRei+UPsvbVizEcr3ec10i8k4v5/zRwLkKFv9L1eMi3L8BSG2ZFPr/wasrrMDZLsHg9zLveJ/JHcArqvq1wFMl//mnuva8f/b5ntWQ7z/cDJG/4GZEfDHf44npmubgZnW8ALzsXxcuX/o48Jp32+BtF9yibOuBF4GWwLk+jivwrQOuyPe1pbnmu3Eh/wHcr68r47xeoMX7z7ke+Baea0Wh/KW4/h9517fG+0KZHtj/i961vEpghlOq/w/ev6lnvPflZ0Blvq85MLaTcOmZNcDz3t95Y+HzT3Ptef3szdLFMAzDiJWxngozDMMwYsaExTAMw4gVExbDMAwjVkxYDMMwjFgxYTEMwzBixYTFKCpEpM9za31BRJ4Vkb/KsP9kEfl0hPP+TkRa4htp8SMid4nIJZn3NIzBmLAYxUaPqi5U1eOAvwf+JcP+k4GMwpIvAh3NhlEymLAYxcwkYAc4ryQRedyLYl4UEd+l+hZgrhflfMXb9/94+7wgIrcEzvdBEXlGRP4iIid7+5aLyFdEZKVn6Pcpb/t0EXnCO+9L/v5BxK2J82XvnM+IyDu87XeJyNdEZAXwZXHrhjzgnf8pEVkQuKZl3ljXiMgHvO1nicifvGv9mecThYjcIiJrvX2/6m37oDe+F0TkiQzXJCLyLe8cv2TAtNEwssJ+LRnFxgQReR6owq1F8R5v+17gIlXtFJGpwFMi8iBuHY5jVHUhgIici7P9PlFVu0WkIXDuClVdLG5RpBuAM3Ed7LtU9QQRqQT+ICKPARfj7D5uFpFyYGKK8XZ65/wocCvwPm/7EcCZqtonIt8EnlPVC0XkPThb9oXA//Ve+1hv7PXetf2Dd+weEfk8cJ2IfAtn3XGUqqqITPZe50vA2aq6KbAt1TUtAo4EjgUOAtYCd0b6VAwjgAmLUWz0BETiXcAPReQYnE3HP4tzce7HWXsfFHL8mcAyVe0GUNXgGia+eeFqYLZ3/yxgQaDWUIfzV1oJ3CnOAPABVX0+xXjvDtz+e2D7z1S1z7t/EvABbzy/FZEpIlLnjXWJf4Cq7hCR9+EWa/qDs4liPPAnoBMnrt/3oo2HvMP+ANwlIvcEri/VNZ0C3O2Nq01EfpvimgwjLSYsRtGiqn/yfsE34nyOGoHjVfWAiGzARTXJCKltv/d5t30M/N8Q4H+q6hDzTU/E3gv8SES+oqo/DBtmivt7ksYUdlzYWAW3GNXSkPEsBs7AidG1wHtU9WoROdEb5/MisjDVNXmRmnk8GSPGaixG0SIiR+GWVG3H/ere5onK6cBh3m5duCVbfR4DPi4iE71zBFNhYTwKXONFJojIEeLcow/zXu97OHfZ5hTHXxq4/VOKfZ4APuKd/zTgbXVrajyGEwj/euuBp4B3B+o1E70x1QB1qvow8Le4VBoiMldVn1bVL+Es0GeluiZvHEu8Gsx04PQM741hhGIRi1Fs+DUWcL+8L/fqFD8GfiEiq3AOr38GUNV2EfmDiLwEPKKq/9v71b5KRPYDDwNfSPN638elxZ4Vl3vajqvRnAb8bxE5AOzGWamHUSkiT+N+xA2JMjxuBJaJyBqgmwGr9/8H3OaNvQ/4R1X9LxH5GHC3Vx8BV3PpAn4uIlXe+/I577mviMjh3rbHcY7Xa1Jc0/24mtWLOJfb/07zvhhGSszd2DByhJeOa1HVt/M9FsMYTSwVZhiGYcSKRSyGYRhGrFjEYhiGYcSKCYthGIYRKyYshmEYRqyYsBiGYRixYsJiGIZhxMr/BypnloWwEWnYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_losses()\n",
    "plt.ylim(0., 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try with xresnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'XResNet' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a6c892e9fc67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                )\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'XResNet' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "learn = Learner(data, model=models.xresnet34(), \n",
    "                opt_func=partial(RAdam),\n",
    "                loss_func=root_mean_squared_error,\n",
    "                wd=1e-3, bn_wd=False, true_wd=True,\n",
    "               )\n",
    "\n",
    "learn.model[-1] = nn.Linear(512, 1, bias=True).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai]",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
